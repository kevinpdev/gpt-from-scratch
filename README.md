# ğŸ§  Train a Small GPT-Style LLM from Scratch

ğŸš€ **This repository contains a Jupyter Notebook that trains a small GPT-style, decoder-only language model from scratch using PyTorch.**

ğŸ”— **[Open the Notebook](./llm-from-scratch.ipynb)**

## ğŸ“Œ Overview

This project is an educational walkthrough of the full process of building and training a **Minimal GPT-style Decoder Only Transformer Model**. The notebook covers:

- ğŸ“– **Tokenization** â€“ Converting text into tokens
- ğŸ”„ **Positional Encoding** â€“ Adding order to input sequences
- ğŸ“ˆ **Self Attention Intuition** - Building intuition behind the self attention operation
- ğŸ— **Transformer Decoder Blocks** â€“ Multi-head self-attention & feedforward layers
- ğŸ¯ **Training from Scratch** â€“ Using a small pretraining and SFT dataset to train a language model
- ğŸ”¥ **Inference** â€“ Generating text using the trained model

## ğŸ“‚ Repository Structure

ğŸ“‚ gpt-from-scratch
â”‚â”€â”€ ğŸ“„ README.md # Project documentation (this file)
â”‚â”€â”€ ğŸ“’ llm-from-scratch.ipynb # Jupyter Notebook with full training pipeline

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/kevinpdev/gpt-from-scratch.git
cd gpt-from-scratch
```

### 2ï¸âƒ£ Install Dependencies

Make sure you have Python and Jupyter installed. Install required packages:

```bash
pip install torch transformers datasets tqdm jupyter
```

### 3ï¸âƒ£ Run the Notebook

Launch Jupyter Notebook:

```bash
jupyter notebook
```

Open llm-from-scratch.ipynb and run

## ğŸ¯ Goals & Use Cases

âœ… Understand dataset formats and working with Huggingface libraries
âœ… Learn the process of tokenization
âœ… Learn the inner workings of GPT-style models
âœ… Train a small-scale Transformer on a custom dataset
âœ… Understand self-attention and language modeling
âœ… Experiment with fine-tuning & inference

## ğŸ”— Notebook & Resources

ğŸ“Œ Notebook: llm-from-scratch.ipynb
ğŸ“– Transformer Paper: [â€œAttention Is All You Need"](https://arxiv.org/pdf/1706.03762)
ğŸ“– GPT Paper: ["Improving Language Understanding by Generative Pre-Training"](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
ğŸ›  PyTorch Documentation: [pytorch.org](https://pytorch.org/)
ğŸ‘ Huggingface Documentation: [https://huggingface.co/docs](https://huggingface.co/docs)
